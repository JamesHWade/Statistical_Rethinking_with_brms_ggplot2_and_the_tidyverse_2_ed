{
  "hash": "4496cdf75fe27e3b2669fcb635ed0fdc",
  "result": {
    "markdown": "---\ntitle: Sampling the Imaginary\n\nexecute: \n  warning: false\n  freeze: auto\n\nfig-align: center\nwidth: 100\nfig-dpi: 150\n\n---\n\n\nIf you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute\n\n$$\\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire) Pr(vampire)}}{\\text{Pr(positive)}}.$$\n\nWe'll do so within a tibble.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ntibble(pr_positive_vampire = .95,\n       pr_positive_mortal  = .01,\n       pr_vampire          = .001) %>% \n  mutate(pr_positive = pr_positive_vampire * pr_vampire + pr_positive_mortal * (1 - pr_vampire)) %>% \n  mutate(pr_vampire_positive = pr_positive_vampire * pr_vampire / pr_positive) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1\nColumns: 5\n$ pr_positive_vampire <dbl> 0.95\n$ pr_positive_mortal  <dbl> 0.01\n$ pr_vampire          <dbl> 0.001\n$ pr_positive         <dbl> 0.01094\n$ pr_vampire_positive <dbl> 0.08683729\n```\n:::\n:::\n\n\nHere's the other way of tackling the vampire problem, this time using the frequency format.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(pr_vampire          = 100 / 100000,\n       pr_positive_vampire = 95 / 100,\n       pr_positive_mortal  = 999 / 99900) %>% \n  mutate(pr_positive = 95 + 999) %>% \n  mutate(pr_vampire_positive = pr_positive_vampire * 100 / pr_positive) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1\nColumns: 5\n$ pr_vampire          <dbl> 0.001\n$ pr_positive_vampire <dbl> 0.95\n$ pr_positive_mortal  <dbl> 0.01\n$ pr_positive         <dbl> 1094\n$ pr_vampire_positive <dbl> 0.08683729\n```\n:::\n:::\n\n\n> The posterior distribution is a probability distribution. And like all probability distributions, we can imagine drawing *samples* from it. The sampled events in this case are parameter values. Most parameters have no exact empirical realization. The Bayesian formalism treats parameter distributions as relative plausibility, not as any physical random process. In any event, randomness is always a property of information, never of the real world. But inside the computer, parameters are just as empirical as the outcome of a coin flip or a die toss or an agricultural experiment. The posterior defines the expected frequency that different parameter values will appear, once we start plucking parameters out of it. [@mcelreathStatisticalRethinkingBayesian2020, p. 50, *emphasis* in the original]\n\n## Sampling from a grid-approximate posterior\n\nOnce again, here we use grid approximation to generate samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many grid points would you like?\nn <- 1000\nn_success <- 6\nn_trials  <- 9\n\n(\n  d <-\n  tibble(p_grid = seq(from = 0, to = 1, length.out = n),\n         # note we're still using a flat uniform prior\n         prior  = 1) %>% \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %>% \n  mutate(posterior = (likelihood * prior) / sum(likelihood * prior))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,000 × 4\n    p_grid prior likelihood posterior\n     <dbl> <dbl>      <dbl>     <dbl>\n 1 0           1   0         0       \n 2 0.00100     1   8.43e-17  8.43e-19\n 3 0.00200     1   5.38e-15  5.38e-17\n 4 0.00300     1   6.11e-14  6.11e-16\n 5 0.00400     1   3.42e-13  3.42e-15\n 6 0.00501     1   1.30e-12  1.30e-14\n 7 0.00601     1   3.87e-12  3.88e-14\n 8 0.00701     1   9.73e-12  9.74e-14\n 9 0.00801     1   2.16e-11  2.16e-13\n10 0.00901     1   4.37e-11  4.38e-13\n# … with 990 more rows\n```\n:::\n:::\n\n\nOur `d` data contains all the components in McElreath's **R** code 3.2 block. Do note we've renamed his `prob_p` and `prob_data` as `prior` and `likelihood`, respectively. Now we'll use the `dplyr::slice_sample()` function to sample rows from `d`, saving them as `sample`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many samples would you like?\nn_samples <- 1e4\n\n# make it reproducible\nset.seed(3)\n\nsamples <-\n  d %>% \n  slice_sample(n = n_samples, weight_by = posterior, replace = T)\n\nglimpse(samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 10,000\nColumns: 4\n$ p_grid     <dbl> 0.5645646, 0.6516517, 0.5475475, 0.5905906, 0.5955956, 0.78…\n$ prior      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ likelihood <dbl> 0.22455994, 0.27190272, 0.20966655, 0.24460869, 0.24799092,…\n$ posterior  <dbl> 0.0022478473, 0.0027217490, 0.0020987643, 0.0024485355, 0.0…\n```\n:::\n:::\n\n\nNow we can plot the left panel of Figure 3.1 with `geom_point()`. But before we do, we'll need to add a variable numbering the samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  mutate(sample_number = 1:n()) %>% \n  \n  ggplot(aes(x = sample_number, y = p_grid)) +\n  geom_point(alpha = 1/10) +\n  scale_y_continuous(\"proportion of water (p)\", limits = c(0, 1)) +\n  xlab(\"sample number\")\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-5-1.png){width=525}\n:::\n:::\n\n\nWe'll make the density in the right panel with `geom_density()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  ggplot(aes(x = p_grid)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion of water (p)\", limits = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-6-1.png){width=525}\n:::\n:::\n\n\nThat was based on `1e4` samples. On page 53, McElreath said the density would converge on the idealized shape if we keep increasing the number of samples. Here's what it looks like with `1e6`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3)\n\nd %>% \n  slice_sample(n = 1e6, weight_by = posterior, replace = T) %>% \n  ggplot(aes(x = p_grid)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion of water (p)\", limits = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-7-1.png){width=525}\n:::\n:::\n\n\nYep, that's more ideal.\n\n## Sampling to summarize\n\n\"Once your model produces a posterior distribution, the model's work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly how it is summarized depends upon your purpose\" (p. 53).\n\n### Intervals of defined boundaries.\n\nTo get the proportion of water less than some value of `p_grid` within the **tidyverse**, you might first `filter()` by that value and then take the `sum()` within `summarise()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  filter(p_grid < .5) %>% \n  summarise(sum = sum(posterior))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    sum\n  <dbl>\n1 0.172\n```\n:::\n:::\n\n\nTo learn more about `dplyr::summarise()` and related functions, check out Baert's [*Data wrangling part 4: Summarizing and slicing your data*](https://suzan.rbind.io/2018/04/dplyr-tutorial-4/) and [Section 5.6](https://r4ds.had.co.nz/transform.html#grouped-summaries-with-summarise) of *R4DS* [@grolemundDataScience2017].\n\nIf what you want is a frequency based on filtering by `samples`, then you might use `n()` within `summarise()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>%\n  filter(p_grid < .5) %>% \n  summarise(sum = n() / n_samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    sum\n  <dbl>\n1 0.163\n```\n:::\n:::\n\n\nA more explicit approach for the same computation is to follow up `count()` with `mutate()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  count(p_grid < .5) %>% \n  mutate(probability = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  `p_grid < 0.5`     n probability\n  <lgl>          <int>       <dbl>\n1 FALSE           8371       0.837\n2 TRUE            1629       0.163\n```\n:::\n:::\n\n\nAn even trickier approach for the same is to insert the logical statement `p_grid < .5` within the `mean()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>%\n  summarise(sum = mean(p_grid < .5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    sum\n  <dbl>\n1 0.163\n```\n:::\n:::\n\n\nMuch like McElreath discussed in the **Overthinking: Counting with `sum`** box, this works \"because R internally converts a logical expression, like `samples < 0.5`, to a vector of `TRUE` and `FALSE` results, one for each element of `samples`, saying whether or not each element matches the criterion\" (p. 54). When we inserted that vector of `TRUE` and `FALSE` values within the `mean()` function, they were then internally converted to a vector of 1's and 0's, the mean of which was the probability. Tricky!\n\nTo determine the posterior probability between 0.5 and 0.75, you can use `&` within `filter()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  filter(p_grid > .5 & p_grid < .75) %>% \n  summarise(sum = n() / n_samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    sum\n  <dbl>\n1 0.606\n```\n:::\n:::\n\n\nJust multiply that value by 100 to get a percent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  filter(p_grid > .5 & p_grid < .75) %>% \n  summarise(sum     = n() / n_samples,\n            percent = n() / n_samples * 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n    sum percent\n  <dbl>   <dbl>\n1 0.606    60.6\n```\n:::\n:::\n\n\nAnd, of course, you can do that with our `mean()` trick, too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>%\n  summarise(percent = 100 * mean(p_grid > .5 & p_grid < .75))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  percent\n    <dbl>\n1    60.6\n```\n:::\n:::\n\n\n### Intervals of defined mass.\n\n> It is more common to see scientific journals reporting an interval of defined mass, usually known as a **confidence interval**. An interval of posterior probability, such as the ones we are working with, may instead be called a **credible interval**. We're going to call it a **compatibility interval** instead, in order to avoid the unwarranted implications of \"confidence\" and \"credibility.\" What the interval indicates is a range of parameter values compatible with the model and data. The model and data themselves may not inspire confidence, in which case the interval will not either. (p. 54, **emphasis** in the original)\n\nAs a part of this block quote, McElreath linked to endnote 54, which reads: \"I learned this term from Sander Greenland and his collaborators. See @amrheinScientistsRiseStatistical2019 and Gelman and Greenland [-@gelmanAreConfidenceIntervals2019]\" (p. 560).\n\nWe'll create the upper two panels for Figure 3.2 with `geom_line()`, `geom_area()`, some careful filtering, and a little patchwork syntax.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# upper left panel\np1 <-\n  d %>% \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  geom_area(data = d %>% filter(p_grid < .5)) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# upper right panel\np2 <-\n  d %>% \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  # note this next line is the only difference in code from the last plot\n  geom_area(data = d %>% filter(p_grid < .75 & p_grid > .5)) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# combine\nlibrary(patchwork)\np1 + p2\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-15-1.png){width=900}\n:::\n:::\n\n\nWe'll come back for the lower two panels in a bit.\n\nSince we saved our `p_grid` samples within the well-named `samples` tibble, we'll have to index with `$` within `quantile`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(q_80 <- quantile(samples$p_grid, prob = .8))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      80% \n0.7627628 \n```\n:::\n:::\n\n\nThat value will come in handy for the lower left panel of Figure 3.2. For an alternative approach, we could `select()` the `samples` vector, extract it from the tibble with `pull()`, and then pump it into `quantile()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  pull(p_grid) %>% \n  quantile(prob = .8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      80% \n0.7627628 \n```\n:::\n:::\n\n\nWe might also use `quantile()` within `summarise()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  summarise(`80th percentile` = quantile(p_grid, p = .8))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  `80th percentile`\n              <dbl>\n1             0.763\n```\n:::\n:::\n\n\nHere's the `summarise()` approach with two probabilities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  summarise(`10th percentile` = quantile(p_grid, p = .1),\n            `90th percentile` = quantile(p_grid, p = .9))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  `10th percentile` `90th percentile`\n              <dbl>             <dbl>\n1             0.451             0.815\n```\n:::\n:::\n\n\nThe **tidyverse** approach is nice in that that family of functions typically returns a data frame. But sometimes you just want your values in a numeric vector for the sake of quick indexing. In that case, base **R** `quantile()` shines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(q_10_and_90 <- quantile(samples$p_grid, prob = c(.1, .9)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      10%       90% \n0.4514515 0.8148148 \n```\n:::\n:::\n\n\nNow we have our cutoff values saved as `q_80` and `q_10_and_90`, we're ready to make the bottom panels of Figure 3.2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lower left panel\np1 <-\n  d %>% \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  geom_area(data = d %>% filter(p_grid < q_80)) +\n  annotate(geom = \"text\",\n           x = .25, y = .0025,\n           label = \"lower 80%\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# lower right panel\np2 <-\n  d %>% \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  geom_area(data = d %>% filter(p_grid > q_10_and_90[1] & p_grid < q_10_and_90[2])) +\n  annotate(geom = \"text\",\n           x = .25, y = .0025,\n           label = \"middle 80%\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# combine\np1 + p2\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-21-1.png){width=900}\n:::\n:::\n\n\nNow we follow along with McElreath's **R** code 3.11 to compute a highly skewed posterior. We've already defined `p_grid` and `prior` within `d`, above. Here we'll reuse them and update the rest of the columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# here we update the `dbinom()` parameters\nn_success <- 3\nn_trials  <- 3\n\n# update `d`\nd <-\n  d %>% \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %>% \n  mutate(posterior  = (likelihood * prior) / sum(likelihood * prior))\n\n# make the next part reproducible\nset.seed(3)\n\n# here's our new samples tibble\n(\n  samples <-\n    d %>% \n    slice_sample(n = n_samples, weight_by = posterior, replace = T)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,000 × 4\n   p_grid prior likelihood posterior\n    <dbl> <dbl>      <dbl>     <dbl>\n 1  0.717     1     0.368   0.00147 \n 2  0.652     1     0.277   0.00111 \n 3  0.548     1     0.164   0.000656\n 4  1         1     1       0.00400 \n 5  0.991     1     0.973   0.00389 \n 6  0.788     1     0.489   0.00195 \n 7  0.940     1     0.830   0.00332 \n 8  0.817     1     0.545   0.00218 \n 9  0.955     1     0.871   0.00348 \n10  0.449     1     0.0908  0.000363\n# … with 9,990 more rows\n```\n:::\n:::\n\n\nThe `rethinking::PI()` function works like a nice shorthand for `quantile()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(samples$p_grid, prob = c(.25, .75))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      25%       75% \n0.7087087 0.9349349 \n```\n:::\n\n```{.r .cell-code}\nrethinking::PI(samples$p_grid, prob = .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      25%       75% \n0.7087087 0.9349349 \n```\n:::\n:::\n\n\nNow's a good time to introduce Matthew Kay's [-@R-tidybayes] [**tidybayes** package](https://mjskay.github.io/tidybayes/), which offers an array of convenience functions for summarizing Bayesian models of the type we'll be working with in this project. For all the **brms**-related deets, see Kay's [-@kayExtractingVisualizingTidy2021] vignette, [*Extracting and visualizing tidy draws from brms models*](https://mjskay.github.io/tidybayes/articles/tidy-brms.html). Here we start simple.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidybayes)\n\nmedian_qi(samples$p_grid, .width = .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          y      ymin      ymax .width .point .interval\n1 0.8428428 0.7087087 0.9349349    0.5 median        qi\n```\n:::\n:::\n\n\nThe **tidybayes** package contains a [family of functions](https://mjskay.github.io/tidybayes/articles/tidy-brms.html#point-summaries-and-intervals) that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With `median_qi()`, we asked for the median and quantile-based intervals--just like we've been doing with `quantile()`. Note how the `.width` argument within `median_qi()` worked the same way the `prob` argument did within `rethinking::PI()`. With `.width = .5`, we indicated we wanted a quantile-based 50% interval, which was returned in the `ymin` and `ymax` columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we'll request 50%, 80%, and 99% intervals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_qi(samples$p_grid, .width = c(.5, .8, .99))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          y      ymin      ymax .width .point .interval\n1 0.8428428 0.7087087 0.9349349   0.50 median        qi\n2 0.8428428 0.5705706 0.9749750   0.80 median        qi\n3 0.8428428 0.2562563 0.9989990   0.99 median        qi\n```\n:::\n:::\n\n\nThe `.width` column in the output indexed which line presented which interval. The value in the `y` column remained constant across rows. That's because that column listed the measure of central tendency, the median in this case.\n\nNow let's use the `rethinking::HPDI()` function to return 50% highest posterior density intervals (HPDIs).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrethinking::HPDI(samples$p_grid, prob = .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     |0.5      0.5| \n0.8418418 0.9989990 \n```\n:::\n:::\n\n\nThe reason I introduce **tidybayes** now is that the functions of the **brms** package only support percentile-based intervals of the type we computed with `quantile()` and `median_qi()`. But **tidybayes** also supports HPDIs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmode_hdi(samples$p_grid, .width = .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         y      ymin     ymax .width .point .interval\n1 0.955384 0.8418418 0.998999    0.5   mode       hdi\n```\n:::\n:::\n\n\nThis time we used the mode as the measure of central tendency. With this family of **tidybayes** functions, you specify the measure of central tendency in the prefix (i.e., `mean`, `median`, or `mode`) and then the type of interval you'd like (i.e., `qi` or `hdi`).\n\nIf all you want are the intervals without the measure of central tendency or all that other technical information, **tidybayes** also offers the handy `qi()` and `hdi()` functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqi(samples$p_grid, .width = .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]      [,2]\n[1,] 0.7087087 0.9349349\n```\n:::\n\n```{.r .cell-code}\nhdi(samples$p_grid, .width = .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]     [,2]\n[1,] 0.8418418 0.998999\n```\n:::\n:::\n\n\nThese are nice in that they return simple numeric vectors, making them particularly useful to use as references within **ggplot2** plots. Now we have that skill, we can use it to make Figure 3.3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lower left panel\np1 <-\n  d %>% \n  ggplot(aes(x = p_grid, y = posterior)) +\n  # check out our sweet `qi()` indexing\n  geom_area(data = d %>% \n              filter(p_grid > qi(samples$p_grid, .width = .5)[1] & \n                       p_grid < qi(samples$p_grid, .width = .5)[2]),\n            fill = \"grey75\") +\n  geom_line() +\n  labs(subtitle = \"50% Percentile Interval\",\n       x = \"proportion of water (p)\",\n       y = \"density\")\n\n# lower right panel\np2 <-\n  d %>% \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_area(data = . %>% \n              filter(p_grid > hdi(samples$p_grid, .width = .5)[1] & \n                       p_grid < hdi(samples$p_grid, .width = .5)[2]),\n            fill = \"grey75\") +\n  geom_line() +\n  labs(subtitle = \"50% HPDI\",\n       x = \"proportion of water (p)\",\n       y = \"density\")\n\n# combine!\np1 | p2\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-29-1.png){width=825}\n:::\n:::\n\n\nIn the `geom_area()` line for the HPDI plot, did you notice how we replaced `d` with `.`? When using the pipe (i.e., `%>%`), you can use the `.` as a placeholder for the original data object. It's an odd and handy trick to know about. Go [here](https://magrittr.tidyverse.org/reference/pipe.html) to learn more.\n\n> So the HPDI has some advantages over the PI. But in most cases, these two types of interval are very similar. They only look so different in this case because the posterior distribution is highly skewed. If we instead used samples from the posterior distribution for six waters in nine tosses, these intervals would be nearly identical. Try it for yourself, using different probability masses, such as `prob=0.8` and `prob=0.95`. When the posterior is bell shaped, it hardly matters which type of interval you use. (p. 57)\n\nLet's try it out. First we'll update the simulation for six waters in nine tosses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \"six waters in nine tosses\"\nn_success <- 6\nn_trials  <- 9\n\nnew_d <-\n  d %>% \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %>% \n  mutate(posterior = (likelihood * prior) / sum(posterior))\nset.seed(3)\n\nnew_samples <-\n  new_d %>% \n  slice_sample(n = n_samples, weight_by = posterior, replace = T)\n```\n:::\n\n\nHere are the intervals by `.width` and type of `.interval`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(mean_hdi(new_samples$p_grid, .width = c(.8, .95)),\n          mean_qi(new_samples$p_grid,  .width = c(.8, .95))) %>% \n  select(.width, .interval, ymin:ymax) %>% \n  arrange(.width) %>% \n  mutate_if(is.double, round, digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  .width .interval ymin ymax\n1   0.80       hdi 0.49 0.84\n2   0.80        qi 0.45 0.81\n3   0.95       hdi 0.37 0.89\n4   0.95        qi 0.35 0.88\n```\n:::\n:::\n\n\nWe didn't need that last `mutate_if()` line. It just made it easier to compare the `ymin` and `ymax` values. Anyway, McElreath was right. This time the differences between the HPDIs and QIs were trivial. Here's a look at the posterior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_d %>% \n  ggplot(aes(x = p_grid)) +\n  geom_line(aes(y = posterior)) +\n  labs(subtitle = \"Six waters in nine tosses made\\nfor a more symmetrical posterior\",\n       x = \"proportion of water (p)\",\n       y = \"density\")\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-32-1.png){width=450}\n:::\n:::\n\n\nBe warned:\n\n> The HPDI also has some disadvantages. HPDI is more computationally intensive than PI and suffers from greater *simulation variance*, which is a fancy way of saying that it is sensitive to how many samples you draw from the posterior. It is also harder to understand and many scientific audiences will not appreciate its features, while they will immediately understand a percentile interval, as ordinary non-Bayesian intervals are typically interpreted (incorrectly) as percentile intervals (pp. 57--58, *emphasis* in the original)\n\nFor convenience, we'll primarily stick to the PI-based intervals in this ebook.\n\n#### Rethinking: What do compatibility intervals mean?\n\nAt the start of this section, McElreath poked a little at frequentist confidence intervals. For an introduction to confidence intervals from the perspective of a frequentist, you might check out Cumming's [-@cummingNewStatisticsWhy2014] [*The new statistics: Why and how*](https://journals.sagepub.com/doi/pdf/10.1177/0956797613504966) and the works referenced therein. Though their definition isn't the most intuitive, I usually use confidence intervals when I wear my frequentist hat.\n\n### Point estimates.\n\nWe've been calling point estimates measures of central tendency. If we `arrange()` our `d` tibble in descending order by `posterior`, we'll see the corresponding `p_grid` value for its MAP estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  arrange(desc(posterior))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,000 × 4\n   p_grid prior likelihood posterior\n    <dbl> <dbl>      <dbl>     <dbl>\n 1  1         1      1       0.00400\n 2  0.999     1      0.997   0.00398\n 3  0.998     1      0.994   0.00397\n 4  0.997     1      0.991   0.00396\n 5  0.996     1      0.988   0.00395\n 6  0.995     1      0.985   0.00394\n 7  0.994     1      0.982   0.00392\n 8  0.993     1      0.979   0.00391\n 9  0.992     1      0.976   0.00390\n10  0.991     1      0.973   0.00389\n# … with 990 more rows\n```\n:::\n:::\n\n\nTo emphasize it, we can use `slice()` to select the top row.\n \n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  arrange(desc(posterior)) %>% \n  slice(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  p_grid prior likelihood posterior\n   <dbl> <dbl>      <dbl>     <dbl>\n1      1     1          1   0.00400\n```\n:::\n:::\n\n\nWe can get the mode with `mode_hdi()` or `mode_qi()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% mode_hdi(p_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  p_grid .lower .upper .width .point .interval\n   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  0.955  0.475      1   0.95 mode   hdi      \n```\n:::\n\n```{.r .cell-code}\nsamples %>% mode_qi(p_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  p_grid .lower .upper .width .point .interval\n   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1  0.955  0.399  0.994   0.95 mode   qi       \n```\n:::\n:::\n\n\nThose returned a lot of output in addition to the mode. If all you want is the mode itself, you can just use `tidybayes::Mode()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMode(samples$p_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.955384\n```\n:::\n:::\n\n\nMedians and means are typical measures of central tendency, too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  summarise(mean   = mean(p_grid),\n            median = median(p_grid))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n   mean median\n  <dbl>  <dbl>\n1 0.803  0.843\n```\n:::\n:::\n\n\nWe can inspect the three types of point estimate in the left panel of Figure 3.4. First we'll bundle the three point estimates together in a tibble.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(\n  point_estimates <-\n  bind_rows(samples %>% mean_qi(p_grid),\n            samples %>% median_qi(p_grid),\n            samples %>% mode_qi(p_grid)) %>% \n  select(p_grid, .point) %>% \n  # these last two columns will help us annotate  \n  mutate(x = p_grid + c(-.03, .03, -.03),\n         y = c(.0005, .0012, .002))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  p_grid .point     x      y\n   <dbl> <chr>  <dbl>  <dbl>\n1  0.803 mean   0.773 0.0005\n2  0.843 median 0.873 0.0012\n3  0.955 mode   0.925 0.002 \n```\n:::\n:::\n\n\nNow plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  ggplot(aes(x = p_grid)) +\n  geom_area(aes(y = posterior),\n            fill = \"grey75\") +\n  geom_vline(xintercept = point_estimates$p_grid) +\n  geom_text(data = point_estimates,\n            aes(x = x, y = y, label = .point),\n            angle = 90) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\") +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-39-1.png){width=525}\n:::\n:::\n\n\nAs it turns out \"*different loss functions imply different point estimates*\" (p. 59, *emphasis* in the original).\n\nLet $p$ be the proportion of the Earth covered by water and $d$ be our guess. If McElreath pays us \\$100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to $d - p$. If we decide $d = .5$, we can compute our expected loss.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  summarise(`expected loss` = sum(posterior * abs(0.5 - p_grid)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  `expected loss`\n            <dbl>\n1           0.313\n```\n:::\n:::\n\n\nWhat McElreath did with `sapply()`, we'll do with `purrr::map()`. If you haven't used it, `map()` is part of a family of similarly-named functions (e.g., `map2()`) from the [**purrr** package](https://purrr.tidyverse.org) [@R-purrr], which is itself part of the [**tidyverse**](https://www.tidyverse.org). The `map()` family is the **tidyverse** alternative to the family of `apply()` functions from the base **R** framework. You can learn more about how to use the `map()` family [here](https://purrr.tidyverse.org/reference/map.html) or [here](https://jennybc.github.io/purrr-tutorial/ls01_map-name-position-shortcuts.html) or [here](https://data.library.virginia.edu/getting-started-with-the-purrr-package-in-r/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_loss <- function(our_d) {\n  d %>% \n    mutate(loss = posterior * abs(our_d - p_grid)) %>% \n    summarise(weighted_average_loss = sum(loss))\n}\n\n(\n  l <-\n  d %>% \n  select(p_grid) %>% \n  rename(decision = p_grid) %>% \n  mutate(weighted_average_loss = purrr::map(decision, make_loss)) %>% \n  unnest(weighted_average_loss) \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,000 × 2\n   decision weighted_average_loss\n      <dbl>                 <dbl>\n 1  0                       0.800\n 2  0.00100                 0.799\n 3  0.00200                 0.798\n 4  0.00300                 0.797\n 5  0.00400                 0.796\n 6  0.00501                 0.795\n 7  0.00601                 0.794\n 8  0.00701                 0.793\n 9  0.00801                 0.792\n10  0.00901                 0.791\n# … with 990 more rows\n```\n:::\n:::\n\n\nNow we're ready for the right panel of Figure 3.4.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this will help us find the x and y coordinates for the minimum value\nmin_loss <-\n  l %>% \n  filter(weighted_average_loss == min(weighted_average_loss)) %>% \n  as.numeric()\n# the plot\nl %>%   \n  ggplot(aes(x = decision, y = weighted_average_loss)) +\n  geom_area(fill = \"grey75\") +\n  geom_vline(xintercept = min_loss[1], color = \"white\", linetype = 3) +\n  geom_hline(yintercept = min_loss[2], color = \"white\", linetype = 3) +\n  ylab(\"expected proportional loss\") +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-42-1.png){width=525}\n:::\n:::\n\n\nWe saved the exact minimum value as `min_loss[1]`, which is 0.8408408. Within sampling error, this is the posterior median as depicted by our `samples`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  summarise(posterior_median = median(p_grid))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  posterior_median\n             <dbl>\n1            0.843\n```\n:::\n:::\n\n\nThe quadratic loss $(d - p)^2$ suggests we should use the mean instead. Let's investigate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# amend our loss function\nmake_loss <- function(our_d) {\n  d %>% \n    mutate(loss = posterior * (our_d - p_grid)^2) %>% \n    summarise(weighted_average_loss = sum(loss))\n}\n\n# remake our `l` data\nl <-\n  d %>% \n  select(p_grid) %>% \n  rename(decision = p_grid) %>% \n  mutate(weighted_average_loss = purrr::map(decision, make_loss)) %>% \n  unnest(weighted_average_loss)\n\n# update to the new minimum loss coordinates\nmin_loss <-\n  l %>% \n  filter(weighted_average_loss == min(weighted_average_loss)) %>% \n  as.numeric()\n\n# update the plot\nl %>%   \n  ggplot(aes(x = decision, y = weighted_average_loss)) +\n  geom_area(fill = \"grey75\") +\n  geom_vline(xintercept = min_loss[1], color = \"white\", linetype = 3) +\n  geom_hline(yintercept = min_loss[2], color = \"white\", linetype = 3) +\n  ylab(\"expected proportional loss\") +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-44-1.png){width=525}\n:::\n:::\n\n\nBased on quadratic loss $(d - p)^2$, the exact minimum value is 0.8008008. Within sampling error, this is the posterior mean of our `samples`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  summarise(posterior_meaan = mean(p_grid))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  posterior_meaan\n            <dbl>\n1           0.803\n```\n:::\n:::\n\n\n> Usually, research scientists don't think about loss functions. And so any point estimate like the mean or MAP that they may report isn't intended to support any particular decision, but rather to describe the shape of the posterior. You might argue that the decision to make is whether or not to accept an hypothesis. But the challenge then is to say what the relevant costs and benefits would be, in terms of the knowledge gained or lost. Usually it's better to communicate as much as you can about the posterior distribution, as well as the data and the model itself, so that others can build upon your work. Premature decisions to accept or reject hypotheses can cost lives. (p. 61)\n\nIn the endnote (62) linked to the end of that quote in the text, McElreath wrote: \"See @hauerHarmDoneTests2004 for three tales from transportation safety in which testing resulted in premature incorrect decisions and a demonstrable and continuing loss of human life\" (p. 561).\n\n## Sampling to simulate prediction\n\nMcElreath's five good reasons for simulation were\n\n1. model design\n2. model checking,\n3. software validation,\n4. research design, and\n5. forecasting.\n\n### Dummy data.\n\nDummy data for the globe tossing model arise from the binomial likelihood. If you let $w$ be a count of water and $n$ be the number of tosses, the binomial likelihood is\n\n$$\\operatorname{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}.$$\n\nLetting $n = 2$, $p(w) = .7$, and $w_\\text{observed} = 0 \\text{ through }2$, the densities are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(n      = 2,\n       `p(w)` = .7,\n       w      = 0:2) %>% \n  mutate(density = dbinom(w, size = n, prob = `p(w)`))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n      n `p(w)`     w density\n  <dbl>  <dbl> <int>   <dbl>\n1     2    0.7     0    0.09\n2     2    0.7     1    0.42\n3     2    0.7     2    0.49\n```\n:::\n:::\n\n\nIf we're going to simulate, we should probably [set our seed](https://stackoverflow.com/questions/13605271/reasons-for-using-the-set-seed-function). Doing so makes the results reproducible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3)\n\nrbinom(1, size = 2, prob = .7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nHere are ten reproducible draws.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3)\n\nrbinom(10, size = 2, prob = .7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2 1 2 2 1 1 2 2 1 1\n```\n:::\n:::\n\n\nNow generate 100,000 (i.e., `1e5`) reproducible dummy observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many would you like?\nn_draws <- 1e5\n\nset.seed(3)\n\nd <- tibble(draws = rbinom(n_draws, size = 2, prob = .7))\n\nd %>% \n  count(draws) %>% \n  mutate(proportion = n / nrow(d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  draws     n proportion\n  <int> <int>      <dbl>\n1     0  9000      0.09 \n2     1 42051      0.421\n3     2 48949      0.489\n```\n:::\n:::\n\n\nAs McElreath mused in the text (p. 63), those simulated `proportion` values are very close to the analytically calculated values in our `density` column a few code blocks up.\n\nHere's the simulation updated so $n = 9$, which we plot in our version of Figure 3.5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3)\nd <- tibble(draws = rbinom(n_draws, size = 9, prob = .7))\n\n# the histogram\nd %>% \n  ggplot(aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"dummy water count\", breaks = 0:4 * 2) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-50-1.png){width=525}\n:::\n:::\n\n\nMcElreath suggested we play around with different values of `size` and `prob`. With the next block of code, we'll simulate nine conditions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_draws <- 1e5\n\nsimulate_binom <- function(n, probability) {\n  set.seed(3)\n  rbinom(n_draws, size = n, prob = probability) \n}\n\nd <-\n  crossing(n           = c(3, 6, 9),\n           probability = c(.3, .6, .9)) %>% \n  mutate(draws = map2(n, probability, simulate_binom)) %>% \n  ungroup() %>% \n  mutate(n           = str_c(\"n = \", n),\n         probability = str_c(\"p = \", probability)) %>% \n  unnest(draws)\n\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  n     probability draws\n  <chr> <chr>       <int>\n1 n = 3 p = 0.3         0\n2 n = 3 p = 0.3         2\n3 n = 3 p = 0.3         1\n4 n = 3 p = 0.3         0\n5 n = 3 p = 0.3         1\n6 n = 3 p = 0.3         1\n```\n:::\n:::\n\n\nLet's plot the simulation results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  ggplot(aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"dummy water count\", breaks = 0:4 * 2) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank()) +\n  facet_grid(n ~ probability)\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-52-1.png){width=900}\n:::\n:::\n\n\n### Model checking.\n\nIf you're new to applied statistics, you might be surprised how often mistakes arise.\n\n#### Did the software work?\n\nLet this haunt your dreams: \"There is no way to really be sure that software works correctly\" (p. 64).\n\nIf you'd like to dive deeper into these dark waters, check out one my favorite talks from StanCon 2018, [*Esther Williams in the Harold Holt Memorial Swimming Pool*](https://youtu.be/pKZLJPrZLhU?t=26285), by the ineffable [Dan Simpson](https://twitter.com/dan_p_simpson). If Simpson doesn't end up drowning you, see Gabry and Simpson's talk at the Royal Statistical Society 2018, [*Visualization in Bayesian workflow*](https://youtu.be/E8vdXoJId8M), a follow-up blog called [*Maybe it's time to let the old ways die; or We broke R-hat so now we have to fix it*](https://statmodeling.stat.columbia.edu/2019/03/19/maybe-its-time-to-let-the-old-ways-die-or-we-broke-r-hat-so-now-we-have-to-fix-it/), and that blog's associated pre-print by @vehtariRanknormalizationFoldingLocalization2019, [*Rank-normalization, folding, and localization: An improved $\\widehat R$ for assessing convergence of MCMC*](https://arxiv.org/abs/1903.08008).\n\n#### Is the model adequate?\n\n> The implied predictions of the model are uncertain in two ways, and it's important to be aware of both.\n>\n> First, there is observation uncertainty. For any unique value of the parameter $p$, there is a unique implied pattern of observations that the model expects. These patterns of observations are the same gardens of forking data that you explored in the previous chapter. These patterns are also what you sampled in the previous section. There is uncertainty in the predicted observations, because even if you know $p$ with certainty, you won’t know the next globe toss with certainty (unless $p = 0$ or $p = 1$).\n>\n> Second, there is uncertainty about $p$. The posterior distribution over $p$ embodies this uncertainty. And since there is uncertainty about $p$, there is uncertainty about everything that depends upon $p$. The uncertainty in $p$ will interact with the sampling variation, when we try to assess what the model tells us about outcomes.\n>\n> We'd like to *propagate* the parameter uncertainty--carry it forward--as we evaluate the implied predictions. All that is required is averaging over the posterior density for $p$, while computing the predictions. For each possible value of the parameter $p$, there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of $p$, then you could average all of these prediction distributions together, using the posterior probabilities of each value of $p$, to get a **posterior predictive distribution**. (pp. 64--65, *emphasis* in the original)\n\nAll this is depicted in Figure 3.6. To get ready to make our version, let's first refresh our original grid approximation `d`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many grid points would you like?\nn <- 1001\nn_success <- 6\nn_trials  <- 9\n\n(\n  d <-\n  tibble(p_grid = seq(from = 0, to = 1, length.out = n),\n         # note we're still using a flat uniform prior\n         prior  = 1) %>% \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %>% \n  mutate(posterior = (likelihood * prior) / sum(likelihood * prior))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,001 × 4\n   p_grid prior likelihood posterior\n    <dbl> <dbl>      <dbl>     <dbl>\n 1  0         1   0         0       \n 2  0.001     1   8.37e-17  8.37e-19\n 3  0.002     1   5.34e-15  5.34e-17\n 4  0.003     1   6.07e-14  6.07e-16\n 5  0.004     1   3.40e-13  3.40e-15\n 6  0.005     1   1.29e-12  1.29e-14\n 7  0.006     1   3.85e-12  3.85e-14\n 8  0.007     1   9.68e-12  9.68e-14\n 9  0.008     1   2.15e-11  2.15e-13\n10  0.009     1   4.34e-11  4.34e-13\n# … with 991 more rows\n```\n:::\n:::\n\n\nWe can make our version of the top of Figure 3.6 with a little tricky `filter`ing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_area(color = \"grey67\", fill = \"grey67\") +\n  geom_segment(data = . %>% \n                 filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)),\n               aes(xend = p_grid, yend = 0, size = posterior),\n               color = \"grey33\", show.legend = F) +\n  geom_point(data = . %>%\n               filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10))) +\n  annotate(geom = \"text\", \n           x = .08, y = .0025,\n           label = \"Posterior probability\") +\n  scale_size_continuous(range = c(0, 1)) +\n  scale_x_continuous(\"probability of water\", breaks = 0:10 / 10) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-54-1.png){width=1050}\n:::\n:::\n\n\nNote how we weighted the widths of the vertical lines by the `posterior` density.\n\nWe'll need to do a bit of wrangling before we're ready to make the plot in the middle panel of Figure 3.6.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_draws <- 1e5\n\nsimulate_binom <- function(probability) {\n  set.seed(3)\n  rbinom(n_draws, size = 9, prob = probability) \n}\n\nd_small <-\n  tibble(probability = seq(from = .1, to = .9, by = .1)) %>% \n  mutate(draws = purrr::map(probability, simulate_binom)) %>% \n  unnest(draws) %>% \n  mutate(label = str_c(\"p = \", probability))\n\nhead(d_small)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  probability draws label  \n        <dbl> <int> <chr>  \n1         0.1     0 p = 0.1\n2         0.1     2 p = 0.1\n3         0.1     0 p = 0.1\n4         0.1     0 p = 0.1\n5         0.1     1 p = 0.1\n6         0.1     1 p = 0.1\n```\n:::\n:::\n\n\nNow we're ready to plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_small %>%\n  ggplot(aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_x_continuous(NULL, breaks = 0:3 * 3) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = \"Sampling distributions\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ label, ncol = 9) \n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-56-1.png){width=1200}\n:::\n:::\n\n\nTo make the plot at the bottom of Figure 3.6, we'll redefine our `samples`, this time including the `w` variable (see the **R** code 3.26 block in the text).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many samples would you like?\nn_samples <- 1e4\n\n# make it reproducible\nset.seed(3)\n\nsamples <-\n  d %>% \n  slice_sample(n = n_samples, weight_by = posterior, replace = T) %>% \n  mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9))\n\nglimpse(samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 10,000\nColumns: 5\n$ p_grid     <dbl> 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.7…\n$ prior      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ likelihood <dbl> 0.22408531, 0.27179502, 0.15128823, 0.24557832, 0.24825668,…\n$ posterior  <dbl> 0.0022408531, 0.0027179502, 0.0015128823, 0.0024557832, 0.0…\n$ w          <dbl> 4, 7, 3, 3, 7, 6, 8, 2, 6, 4, 5, 5, 8, 6, 4, 6, 8, 2, 6, 9,…\n```\n:::\n:::\n\n\nHere's our histogram.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  ggplot(aes(x = w)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"number of water samples\",\n                     breaks = 0:3 * 3) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(0, 9),\n                  ylim = c(0, 3000)) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-58-1.png){width=450}\n:::\n:::\n\n\nIn Figure 3.7, McElreath considered the longest sequence of the sample values. We've been using `rbinom()` with the size parameter set to 9 for our simulations. E.g.,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(10, size = 9, prob = .6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 7 5 6 8 7 5 6 3 3 4\n```\n:::\n:::\n\n\nNotice this collapsed (i.e., aggregated) over the sequences within the individual sets of 9. What we need is to simulate nine individual trials many times over. For example, this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(9, size = 1, prob = .6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 1 1 1 0 0 0 0 0\n```\n:::\n:::\n\n\nwould be the disaggregated version of just one of the numerals returned by `rbinom()` when `size = 9`. So let's try simulating again with un-aggregated samples. We'll keep adding to our `samples` tibble. In addition to the disaggregated `draws` based on the $p$ values listed in `p_grid`, we'll also want to add a row index for each of those `p_grid` values--it'll come in handy when we plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make it reproducible\nset.seed(3)\n\nsamples <-\n  samples %>% \n  mutate(iter  = 1:n(),\n         draws = purrr::map(p_grid, rbinom, n = 9, size = 1)) %>% \n  unnest(draws)\n\nglimpse(samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 90,000\nColumns: 7\n$ p_grid     <dbl> 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.5…\n$ prior      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ likelihood <dbl> 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.22…\n$ posterior  <dbl> 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.00224…\n$ w          <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3,…\n$ iter       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,…\n$ draws      <int> 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,…\n```\n:::\n:::\n\n\nThe main action is in the `draws` column.\n\nNow we have to count the longest sequences. The base **R** [`rle()` function](https://www.rdocumentation.org/packages/base/versions/3.3/topics/rle) will help with that. Consider McElreath's sequence of tosses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntosses <- c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")\n```\n:::\n\n\nYou can plug that into `rle()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrle(tosses)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRun Length Encoding\n  lengths: int [1:7] 1 1 3 1 1 1 1\n  values : chr [1:7] \"w\" \"l\" \"w\" \"l\" \"w\" \"l\" \"w\"\n```\n:::\n:::\n\n\nFor our purposes, we're interested in `lengths`. That tells us the length of each sequences of the same value. The `3` corresponds to our run of three `w`s. The `max()` function will help us confirm it's the largest value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrle(tosses)$lengths %>% max()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n:::\n\n\nNow let's apply our method to the data and plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  group_by(iter) %>% \n  summarise(longest_run_length = rle(draws)$lengths %>% max()) %>% \n  \n  ggplot(aes(x = longest_run_length)) +\n  geom_histogram(aes(fill = longest_run_length == 3),\n                 binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_fill_viridis_d(option = \"D\", end = .9) +\n  scale_x_continuous(\"longest run length\", breaks = 0:3 * 3) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-65-1.png){width=525}\n:::\n:::\n\n\nLet's look at `rle()` again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrle(tosses)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRun Length Encoding\n  lengths: int [1:7] 1 1 3 1 1 1 1\n  values : chr [1:7] \"w\" \"l\" \"w\" \"l\" \"w\" \"l\" \"w\"\n```\n:::\n:::\n\n\nWe can use the length of the output (i.e., 7 in this example) as the numbers of switches from, in this case, \"w\" and \"l\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrle(tosses)$lengths %>% length()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7\n```\n:::\n:::\n\n\nWith that new trick, we're ready to make the right panel of Figure 3.7.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples %>% \n  group_by(iter) %>% \n  summarise(longest_run_length = rle(draws)$lengths %>% length()) %>% \n  \n  ggplot(aes(x = longest_run_length)) +\n  geom_histogram(aes(fill = longest_run_length == 7),\n                 binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_fill_viridis_d(option = \"D\", end = .9) +\n  scale_x_continuous(\"number of switches\", breaks = 0:3 * 3) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-68-1.png){width=525}\n:::\n:::\n\n\n## ~~Summary~~ Let's practice with **brms**\n\nOpen **brms**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\n```\n:::\n\n\nWith **brms**, we'll fit the primary model of $w = 6$ and $n = 9$ much like we did at the end of [Chapter 2][Markov chain Monte Carlo].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb3.1 <-\n  brm(data = list(w = 6), \n      family = binomial(link = \"identity\"),\n      w | trials(9) ~ 0 + Intercept,\n      # this is a flat prior\n      prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      iter = 5000, warmup = 1000,\n      seed = 3,\n      file = \"fits/b03.01\")\n```\n:::\n\n\nWe'll learn more about the beta distribution in [Chapter 12][Monsters and Mixtures]. But for now, here's the posterior summary for `b_Intercept`, the probability of a \"w\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(b3.1)[\"b_Intercept\", ] %>% \n  round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Estimate Est.Error      Q2.5     Q97.5 \n     0.64      0.14      0.35      0.87 \n```\n:::\n:::\n\n\nAs we'll fully cover in the next chapter, `Estimate` is the posterior mean, the two `Q` columns are the quantile-based 95% intervals, and `Est.Error` is the posterior standard deviation.\n\nMuch like the way we used the `samples()` function to simulate probability values, above, we can do so with the `brms::fitted()` function. But we will have to specify `scale = \"linear\"` in order to return results in the probability metric. By default, `brms::fitted()` will return summary information. Since we want actual simulation draws, we'll specify `summary = F`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <-\n  fitted(b3.1, \n         summary = F,\n         scale = \"linear\") %>% \n  data.frame() %>% \n  set_names(\"p\")\n\nglimpse(f)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 16,000\nColumns: 1\n$ p <dbl> 0.6878563, 0.5386513, 0.7030050, 0.6889854, 0.4738290, 0.5879676, 0.…\n```\n:::\n:::\n\n\nBy default, we have a generically-named vector of 4,000 samples. We'll explain the defaults in later chapters. For now, notice we can view these in a density.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf %>% \n  ggplot(aes(x = p)) +\n  geom_density(fill = \"grey50\", color = \"grey50\") +\n  annotate(geom = \"text\", x = .08, y = 2.5,\n           label = \"Posterior probability\") +\n  scale_x_continuous(\"probability of water\",\n                     breaks = c(0, .5, 1),\n                     limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-72-1.png){width=1050}\n:::\n:::\n\n\nLooks a lot like the posterior probability density at the top of Figure 3.6, doesn't it? Much like we did with `samples`, we can use this distribution of probabilities to predict histograms of `w` counts. With those in hand, we can make an analogue to the histogram in the bottom panel of Figure 3.6.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# the simulation\nset.seed(3)\n\nf <-\n  f %>% \n  mutate(w = rbinom(n(), size = n_trials,  prob = p))\n\n# the plot\nf %>% \n  ggplot(aes(x = w)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"number of water samples\", breaks = 0:3 * 3) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 5000)) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](03_files/figure-html/unnamed-chunk-73-1.png){width=450}\n:::\n:::\n\n\nAs you might imagine, we can use the output from `fitted()` to return disaggregated batches of 0's and 1's, too. And we could even use those disaggregated 0's and 1's to examine longest run lengths and numbers of switches as in the analyses for Figure 3.7. I'll leave those as exercises for the interested reader.\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] brms_2.18.0     Rcpp_1.0.9      tidybayes_3.0.2 patchwork_1.1.2\n [5] forcats_0.5.2   stringr_1.5.0   dplyr_1.0.10    purrr_1.0.0    \n [9] readr_2.1.3     tidyr_1.2.1     tibble_3.1.8    ggplot2_3.4.0  \n[13] tidyverse_1.3.2\n\nloaded via a namespace (and not attached):\n  [1] readxl_1.4.1         backports_1.4.1      plyr_1.8.8          \n  [4] igraph_1.3.5         splines_4.2.2        svUnit_1.0.6        \n  [7] crosstalk_1.2.0      TH.data_1.1-1        rstantools_2.2.0    \n [10] inline_0.3.19        digest_0.6.31        htmltools_0.5.4     \n [13] rethinking_2.21      fansi_1.0.3          magrittr_2.0.3      \n [16] checkmate_2.1.0      googlesheets4_1.0.1  tzdb_0.3.0          \n [19] modelr_0.1.9         RcppParallel_5.1.5   matrixStats_0.63.0  \n [22] xts_0.12.2           sandwich_3.0-2       timechange_0.1.1    \n [25] prettyunits_1.1.1    colorspace_2.0-3     rvest_1.0.3         \n [28] ggdist_3.2.0         haven_2.5.1          xfun_0.36           \n [31] callr_3.7.3          crayon_1.5.2         jsonlite_1.8.4      \n [34] lme4_1.1-31          survival_3.4-0       zoo_1.8-11          \n [37] glue_1.6.2           gtable_0.3.1         gargle_1.2.1        \n [40] emmeans_1.8.3        distributional_0.3.1 pkgbuild_1.3.1      \n [43] rstan_2.21.7         shape_1.4.6          abind_1.4-5         \n [46] scales_1.2.1         mvtnorm_1.1-3        DBI_1.1.3           \n [49] miniUI_0.1.1.1       viridisLite_0.4.1    xtable_1.8-4        \n [52] HDInterval_0.2.4     stats4_4.2.2         StanHeaders_2.21.0-7\n [55] DT_0.26              htmlwidgets_1.6.1    httr_1.4.4          \n [58] threejs_0.3.3        arrayhelpers_1.1-0   posterior_1.3.1     \n [61] ellipsis_0.3.2       pkgconfig_2.0.3      loo_2.5.1           \n [64] farver_2.1.1         dbplyr_2.2.1         utf8_1.2.2          \n [67] tidyselect_1.2.0     labeling_0.4.2       rlang_1.0.6         \n [70] reshape2_1.4.4       later_1.3.0          munsell_0.5.0       \n [73] cellranger_1.1.0     tools_4.2.2          cli_3.5.0           \n [76] generics_0.1.3       broom_1.0.2          evaluate_0.19       \n [79] fastmap_1.1.0        yaml_2.3.6           processx_3.8.0      \n [82] knitr_1.41.8         fs_1.5.2             nlme_3.1-160        \n [85] projpred_2.2.2       mime_0.12            xml2_1.3.3          \n [88] compiler_4.2.2       bayesplot_1.10.0     shinythemes_1.2.0   \n [91] rstudioapi_0.14      gamm4_0.2-6          reprex_2.0.2        \n [94] stringi_1.7.8        ps_1.7.2             Brobdingnag_1.2-9   \n [97] lattice_0.20-45      Matrix_1.5-1         nloptr_2.0.3        \n[100] markdown_1.4         shinyjs_2.1.0        tensorA_0.36.2      \n[103] vctrs_0.5.1          pillar_1.8.1         lifecycle_1.0.3     \n[106] bridgesampling_1.1-2 estimability_1.4.1   httpuv_1.6.7        \n[109] R6_2.5.1             promises_1.2.0.1     gridExtra_2.3       \n[112] codetools_0.2-18     boot_1.3-28          colourpicker_1.2.0  \n[115] MASS_7.3-58.1        gtools_3.9.4         assertthat_0.2.1    \n[118] withr_2.5.0          shinystan_2.6.0      multcomp_1.4-20     \n[121] mgcv_1.8-41          parallel_4.2.2       hms_1.1.2           \n[124] grid_4.2.2           minqa_1.2.5          coda_0.19-4         \n[127] rmarkdown_2.19       cmdstanr_0.5.3       googledrive_2.0.0   \n[130] shiny_1.7.4          lubridate_1.9.0      base64enc_0.1-3     \n[133] dygraphs_1.1.1.6    \n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}